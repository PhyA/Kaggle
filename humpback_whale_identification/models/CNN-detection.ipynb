{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Box Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress annoying stderr output when importing keras.\n",
    "import sys\n",
    "old_stderr = sys.stderr\n",
    "sys.stderr = open('/dev/null', 'w')\n",
    "import keras\n",
    "sys.stderr = old_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the cropping dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-b70f637dafb2>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b70f637dafb2>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    data = [(p, [(int(coord[i]), int(coord[i+1])) for i in range(0, len(coord), 2)]) for p, *coord in data]\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with open('../data/cropping.txt', 'rt') as f: \n",
    "    data = f.read().split('\\n')[:-1]\n",
    "    data = [line.split(',') for line in data]\n",
    "    data = [(p, [(int(coord[i]), int(coord[i+1])) for i in range(0, len(coord), 2)]) for p, *coord in data]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as pil_image\n",
    "from PIL.ImageDraw import Draw\n",
    "from os.path import isfile\n",
    "\n",
    "def expand_path(p):\n",
    "    if isfile('data/train/' + p):\n",
    "        return 'data/train/' + p\n",
    "    if isfile('data/test/' + p):\n",
    "        return 'data/test' + p\n",
    "\n",
    "def read_raw_image(p):\n",
    "    print(p)\n",
    "    p = expand_path(p)\n",
    "    print(p)\n",
    "    return pil_image.open(expand_path(p))\n",
    "\n",
    "def draw_dot(draw, x, y):\n",
    "    draw.ellipse(((x - 5,  y - 5)), fill='red', outline='red')\n",
    "\n",
    "def draw_dots(draw, coordinates):\n",
    "    for x, y in coordinates:\n",
    "        draw_dot(draw, x, y)\n",
    "\n",
    "def bounding_rectangle(boxes):\n",
    "    x0, y0 = boxes[0]\n",
    "    x1, y1 = x0, y0\n",
    "    for x, y in boxes[1:]:\n",
    "        x0 = min(x0, x)\n",
    "        y0 = min(y0, x)\n",
    "        x1 = max(x1, x)\n",
    "        y1 = max(y1, y)\n",
    "    return x0, y0, x1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename, coordinates = data[0]\n",
    "box = bounding_rectangle(coordinates)\n",
    "img = read_raw_image(filename)\n",
    "draw = Draw(img)\n",
    "draw_dots(draw, coordinates)\n",
    "draw.rectangle(box, outline='red')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images are preprocessed by:\n",
    "1. Converting to black&white;\n",
    "2. Compressing horizontally by a factor of 2.15(the mean aspect ratio)\n",
    "3. Apply a random image transformation(only for training)\n",
    "4. Resizing to 128x128;\n",
    "5. Normalizing to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define useful constants\n",
    "img_shape = (128, 128, 1)\n",
    "anisotropy = 2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage import affine_transform\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Read an image as black&white numpy array\n",
    "def read_array(p):\n",
    "    img = read_raw_image(p).convert('L')\n",
    "    return img_to_array(img)\n",
    "\n",
    "def build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    rotation = np.deg2rad(rotation)\n",
    "    shear = np.deg2rad(shear)\n",
    "    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0],\n",
    "                                [-np.sin(rotation), np.cos(rotation), 0],\n",
    "                                [0, 0, 1]])\n",
    "    shift_matrix = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n",
    "    shear_matrix = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n",
    "    zoom_matrix = np.array([[1.0/height_zoom, 0, 0], [0, 1, -width_shift], [0, 0, 1]])\n",
    "    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "\n",
    "# Compute the coordinate transformation required to center the pictures, padding as required.\n",
    "def center_transform(affine, input_shape):\n",
    "    hi, wi = float(input_shape[0]), float(input_shape[1])\n",
    "    ho, wo = float(img_shape[0]), float(img_shape[1])\n",
    "    top, left, bottom, right = 0, 0, hi, wi\n",
    "    if wi/hi/anisotropy < wo/ho:   # input image too narrow, extend width\n",
    "        w = hi*wo/ho*anisotropy\n",
    "        left = (wi - w)/2\n",
    "        right = left + w\n",
    "    else: # input image too wide, extend height\n",
    "        h = wi*ho/wo/anisotropy\n",
    "        top = (hi-h)/2\n",
    "        bottom = top + h\n",
    "    center_matrix = np.array([[1, 0, -h0/2], [0, 1, wo/2], [0, 0, 1]])\n",
    "    scale_matrix = np.array([[(bottom - top)/ho, 0, 0], [0, (right - left)/wo, 0], [0, 0, 1]])\n",
    "    decent_matrix = np.array([[1, 0, hi/2], [0, 1, wi/2], [0, 0, 1]])\n",
    "    return np.dot(np.dot(decent_matrix, scale_matrix), np.dot(affine, center_matrix))\n",
    "\n",
    "# Apply an affine tranformation to an image represented as a numpy array.\n",
    "def transform_img(x, affine):\n",
    "    matrix = affine[:2, :2]\n",
    "    offset = affine[:2, 2]\n",
    "    x = np.moveaxis(x, -1, 0)\n",
    "    channels = [affine_transform(channel, matrix, offset, output_shape=img_shape[:-1], order=1,\n",
    "                                mode='constant', cval=np.average(channel)) for channel in x]\n",
    "    return np.moveaxis(np.stack(channels, axis=0), 0, -1)\n",
    "\n",
    "# Read an image for validation, i.e. without data augmentation.\n",
    "def read_for_validation(p):\n",
    "    x = read_array(p)\n",
    "    t = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    t = center_transform(t, x.shape)\n",
    "    x = transform_img(x, t)\n",
    "    x -= np.mean(x, keepdims=True)\n",
    "    x /= np.std(x, keepdims=True) + K.epsilon()\n",
    "    return x, t\n",
    "\n",
    "# Read an image for training, i.e. including a random affine transformation\n",
    "def read_for_training(p):\n",
    "    x = read_array(p)\n",
    "    t = build_transform(\n",
    "        random.uniform(-5, 5),\n",
    "        random.uniform(-5, 5),\n",
    "        random.uniform(0.9, 1.0), \n",
    "        random.uniform(0.9, 1.0), \n",
    "        random.uniform(-0.05*img_shape[0], 0.05*img_shape[0]), \n",
    "        random.uniform(-0.05*img_shape[1], 0.05*img_shape[1]))\n",
    "    t = center_transform(t, x.shape)\n",
    "    x = transform_img(x, t)\n",
    "    x -= np.mean(x, keepdims=True)\n",
    "    x /= np.std(x, keepdims=True) + K.epsilon()\n",
    "    return x, t\n",
    "\n",
    "# Tranform coordinates according to the provided affine transformation\n",
    "def coord_transorm(boxes, trans):\n",
    "    result = []\n",
    "    for x, y in boxes:\n",
    "        y, x, _ = trans.dot([y, x, 1]).astype(np.int)\n",
    "        result.append((x, y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 128, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 128, 128, 64) 5248        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 128, 128, 64) 256         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 128, 128, 64) 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 64, 64, 64)   16448       dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 64)   256         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 64, 64, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 32, 32, 64)   16448       dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 32, 32, 64)   36928       conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 32, 32, 64)   36928       conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 64)   256         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 32, 32, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 16, 16, 64)   16448       dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 16, 16, 64)   36928       conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 16, 16, 64)   36928       conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 64)   256         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 16, 16, 64)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 64)     16448       dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 64)     36928       conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 64)     36928       conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 64)     256         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 8, 8, 64)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 4, 64)     16448       dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 4, 64)     36928       conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 4, 64)     36928       conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 64)     256         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 4, 4, 64)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 4, 1, 64)     0           dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 1, 4, 64)     0           dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 256)          0           max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 256)          0           max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 256)          0           flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 256)          0           flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 16)           4112        dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 16)           4112        dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32)           0           dense_16[0][0]                   \n",
      "                                                                 dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 32)           0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 4)            132         dropout_49[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 503,588\n",
      "Trainable params: 502,820\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.engine.topology import Input\n",
    "from keras.layers import BatchNormalization, Concatenate, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def conv_bn_dp(x, with_dropout, conv_drop):\n",
    "    x = Conv2D(filters=64, kernel_size=(2, 2), activation='relu', padding='same', strides=2)(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if with_dropout:\n",
    "#       x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n",
    "        x = Dropout(conv_drop)(x)\n",
    "    return x\n",
    "\n",
    "def build_model(with_dropout=True):\n",
    "    kwargs = {'activation': 'relu', 'padding': 'same'}\n",
    "    conv_drop = 0.2\n",
    "    dense_drop = 0.\n",
    "    inp = Input(shape=img_shape)\n",
    "    \n",
    "    x = inp\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(9, 9), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if with_dropout:\n",
    "        x = Dropout(conv_drop)(x)\n",
    "    \n",
    "    for i in range(5):\n",
    "        x = conv_bn_dp(x, with_dropout, conv_drop)\n",
    "    \n",
    "    h = MaxPooling2D(pool_size=(1, int(x.shape[2])))(x)\n",
    "    h = Flatten()(h)\n",
    "    if with_dropout:\n",
    "        h = Dropout(dense_drop)(h)\n",
    "    h = Dense(16, activation='relu')(h)\n",
    "    \n",
    "    v = MaxPooling2D(pool_size=(int(x.shape[1]), 1))(x)\n",
    "    v = Flatten()(v)\n",
    "    if with_dropout:\n",
    "        v = Dropout(dense_drop)(v)\n",
    "    v = Dense(16, activation='relu')(v)\n",
    "    \n",
    "    x = Concatenate()([h, v])\n",
    "    if with_dropout:\n",
    "        x = Dropout(0.5)(x)\n",
    "    x = Dense(4, activation='linear')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "model = build_model(with_dropout=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-py3",
   "language": "python",
   "name": "kaggle-rsna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
